# 1、业务介绍

报账业务，有一个报账三要素：

①系统来源，例如到餐团购，酒店预定，门票等。

②业务类型，描述不同业务的具体的业务场景，例如到餐团购消费场景、到餐团购退款场景。

③明细业务类型，具体的业务场景下 发生的资金明细名称，例如代收商家款、佣金、满减美团承担、满减商家承担、应付商家款、实付商家款。



# 2、业务流程

## 2.1 整体流程

1、上游的交易数据可以通过MQ消息或者RPC接口调用发给我们，我们收到数据后，会先对数据做一个检验，比如必传字段是否为空，然后就是做一个幂等性校验，对于MQ消息就是通过buzLineId、customerId、traceId查询业务数据表，如果存在表示重复数据，不做处理，否则将业务数据落库。RPC接收的数据通过buzLineId、customerId、impKeyNumber来判断。

在计费中，通过kafka发送的消息，经过幂等性校验后，会注册延迟数据清理任务，发送一个延迟消息，在指定时间后将数据从msc_clear_record删除。通用接口pushDate，数据从上游发过来，这个注册延迟任务的方法为空实现，即不删除数据

2、然后进行数据标准化的流程。通过buzLine来判断是否走编排，以及走几层编排。例如门票就是走一层编排，硬编码的方式进行数据标准化，然后走第二层规则进行业财翻译。标准化是通过线程池异步执行

3、根据标准化的返回结果，如果标准化失败，那么将数据插入到重试任务队列中(task_retry_recode表），在crane上会有定时任务，定时执行重试任务队列中的任务，如果再次执行失败，就将重试次数+1，如果执行成功，就将重试任务删除。重试次数达到上限会进行告警

4、调用deliverrule的接口进行数据标准化，拿到标准化后的数据。通过标准化后的数据中taskQueueModels，构建任务队列TaskQueueEntity，然后将TaskQueueEntity（task_queue）和standardBizDataList落库。将标准数据以task任务的维度进行拆分

5、遍历TaskQueueEntityList，发送延迟MQ，延迟5s。如果发送成功，直接退出。如果发送失败，往task_retry_recode表中插入数据（通过TaskQueueEntity构造出taskRetryRecode数据）。消费者收到延迟消息后，通过buzLine、customerId、taskQueueId查询到该任务，然后执行业财翻译的操作。如果该任务的状态为success，表示报账成功，不执行报账，然后将数据从重试任务表删除。否则执行业财翻译，如果执行成功，将该数据从重试任务表删除，否则重试次数+1	

之前的流程是，定时任务每5分钟执行一次重试，失败重试次数+1，成功就删除，重试次数一旦达到26就告警，因此告警信息非常多

现在的流程是，定时任务每5分钟执行一次重试，失败重试次数+1，成功就删除，不会执行告警。新建一个定时任务，每一小时执行一次，通过Lion配置的taskType，扫描对应taskType的task_retry_recode记录，如果重试次数达到上限，执行一次告警



**注意：**

1、定时任务会扫描task_retry_recode表，将这里边的任务进行重试操作，重试成功就删除，重试失败就将重试次数+1，达到阈值（26次）就进行告警。重试时，重新执行数据标准化，业财翻译的流程

2、**数据标准化失败**、**发送MQ失败**、**业财翻译失败**都会将数据插入task_retry_recode表进行重试操作，成功后会删除。从而保证报账数据一定不会丢。消费者收到的消息是buzLine、customerId、taskQueueId，通过这三个参数查询到该任务，判断任务状态，如果是成功，就不报账，避免重复报账。

3、每个业务数据来了后，经过标准化，会拆分为多个task，然后针对task发送MQ，MQ内容就是buzLine、customerId、taskQueueId，如果MQ发送失败，会将这个业务数据直接插入task_retry_recode表中。也就是说标准数据中有一个任务失败就会将这个业务数据整体插入task_retry_recode表。定时任务执行时，会重新进行标准化，然后发送MQ的操作，消费者收到消息后，会查询当前这个任务是否执行成功，成功了就不报账，然后将数据从重试任务表删除。也就避免了重复报账的问题







## 2.2 数据标准化流程

1、通过本地缓存获取所有的编排，首先通过apiModel（通用报账、计费、手工账）过滤，然后通过路由条件找到对应的编排，一般是通过buzLine，businessScenne来判断。如果命中的编排只有一条，那么直接通过，如果命中多条，如果在白名单中配置了，可以通过，否则抛异常

2、执行编排中的DSL语句（从业务数据中取值，例如amount, orderId, traceTime）, 得到结果, 并将结果中的数据转为Map。将结果中的数据分为businessProperties（标准因子，每条数据都要有）和amountSubtypeMap（金额列表）。每个amountType都会对应标准因子，因此一条数据有多个报账任务。

一个编排会有多个业务类型，即多个amountType。例如点评盲盒这个需求，编排的金额列表有 代币业务提现支付amount， 代币业务提现个税amount*0.2/0.8，代币业务提现费用amount+amount*0.2/0.8， 每个类型又有原币和人民币，因此经过第一层编排后，会计算对应的金额，对于点评盲盒，会有3个报账任务。即将所有的标准因子和一个amountType组装为一个task。

3、将报账任务类型、报账日期、标准数据，封装为TaskQueueModel，最终返回







## 2.3 业财翻译

1、通过buzLine进行过滤，得到BusinessTypeRouter（规则）的一个列表。对BusinessTypeRouter进行过滤，状态生效，时间在启用时间之后，版本号最大，然后遍历标准数据task任务，从规则中过滤出符合条件的一个（buzLine、traceType等）。如果只有一个规则符合，直接通过。如果符合的规则大于1，判断是否在白名单，不在就抛异常。

有的数据就会命中多条规则，例如微信小店，就会命中多条规则

2、根据命中的规则获取对应的明细业务路由列表recordNameRouterList，对明细业务类型进行过滤，只保留与当前金额类型（amountType）一致 且 判断条件通过 的路由，如果有多条，也需要配置白名单

有的数据对一个规则会报多条账，例如企微返现的，一个数据命中规则后，明细业务有 返现确认费用，评价返现付款，这两条都要报，所以会配白名单

3、根据明细业务类型，进行报账，会从标准数据中取数，取原币、人民币等数据







## 2.4 返现需求

- 点评盲盒代币提现：大众点评上评价后会进行返现
- 医疗企微返现：在美团加企业微信进行返现
- 特团到餐返现：团购7天核销多少单进行返现
- 微信私域红包



1、因为有很多返现需求，所以希望能够设计出一套通用的逻辑，方便后续返现需求的迭代。所以采用了策略模式来实现

2、实现了三个策略

- FilterNewVersionStrategyProcessor：match方法用于判断上游appkey、报账数据表businessTable（默认为common，如果一个服务有多个返现需求，那么就必须指定这个字段）、businessScenne（业务场景）是否匹配。匹配的话，执行filte方法，当前默认返回true，目的是便于后续的扩展
- FillParametersStrategyProcessor：match方法用于判断appkey、businessTable、businessScenne是否匹配。匹配的话执行fillParameters方法，用于填充参数，例如上游不传递customerId，那我们默认赋值0。
- BusinessSceneRewriteStrategyProcessor：match方法用于判断appkey、businessTable、businessScenne是否匹配，匹配的话执行rewriteBusinessScene方法，用于拼接appkey、businessTable、businessScenne，组合为新的businessScenne。通过这个businessScenne我们可以命中唯一的编排

3、然后走后续的两层编排进行报账。编排判断条件为buzLine，businessScenne，规则判断条件为buzLine、traceType（流水类型：返现）、cashBackType（返现类型：例如TTFX，DZYL）

4、红包的**系统来源**是DAOZONG，有三种**业务类型**：红包发放，用户领取成功，剩余红包退款，因此配置三个编排，每个编排中就有这个类型的原币和人民币。**明细业务类型**就是第二层规则中的明细业务路由。报账三要素就体现在了这两层编排之中。







## 2.5 微信小店需求

1、该需求是MTP业务线，即门票。是美团在微信视频号直播买旅游门票

2、该业务只走第二层规则，因此我们需要通过硬编码将业务数据转为标准数据。走的是计费链路，接受kafka消息进行消费。

3、本次需求添加extendSource字段，字段取值为WX_SHOP。同时如果是微信小店业务，需要将清晰化标识设为1







## 2.6 数仓优化

### 2.6.1 Hive介绍

1、数据仓库是一个用于存储、分析、报告的数据系统，目的是构建面向分析的集成化数据环境。数据仓库本身不生产任何数据，其数据来源于不同外部系统，例如数据库。通过ETL将数据库数据存储到Hive中

2、Hive可以将存储在Hadoop文件系统中的结构化、半结构换数据文件映射为一张数据库表，并基于表提供了一种类似SQL的查询模式，称为Hive SQL(HQL)。Hive的核心是将HQL转换为MapReduce程序，然后将程序提交到Hadoop集群执行。Hive默认引擎是MR，但MR的执行效率太低，一般会换为spark，spark是一个通用的处理大规模数据的分析引擎，即 spark 是一个计算引擎，而不是存储引擎，其本身并不负责数据存储

3、Hive中的数据可以在粒度级别上分为三类：**表**，**分区**，**分桶**。每个分区的数据存储在HDFS上的一个文件夹。分桶表根据哈希函数将一个文件中的数据散列到多个文件中

4、hive中分区就是分目录，将一个大的数据集根据业务分割成小的数据集。hive首先为这个数据库创建了一个文件夹，然后为每一个分区创建一个子目录，将属于各自分区的数据就存放在各自分区的目录下。分区字段是虚拟字段，不存储在文件中

![img](https://cdn.nlark.com/yuque/0/2024/png/40369427/1722431907780-648ce998-3237-450b-8af7-581f4c1123ff.png)![img](https://cdn.nlark.com/yuque/0/2024/png/40369427/1722431924029-bbed5f2d-0782-4839-8041-4f63e7fe9fd2.png)

5、在Hive中，有一个表，定义包括一个名为`dt`的分区键代表日期。将数据加载到这个表中时，会指定每个记录对应的`dt`值，Hive会自动将数据放入对应的分区目录中。这些分区通常在底层文件系统（如HDFS）中表示为子目录，每个子目录的名称都是`dt=yyyy-MM-dd`的形式，其中`yyyy-MM-dd`是日期。

```sql
INSERT INTO sales PARTITION (dt='2023-04-01') VALUES (1, 101, 1000.00, '2023-04-01');
```





### 2.6.2 小文件问题

在按天分区的表中，当数据处理作业（如Hive查询或Spark作业）并行执行时，每个并行任务通常会将其处理结果写入一个单独的文件。这些文件随后会存放在对应日期的分区文件夹下。

**具体步骤**

1. 分区：数据表按天进行分区，每一天的数据存储在对应的分区目录中。 
2. 并行处理：当执行写入操作时（比如INSERT或写入DataFrame），数据处理框架会根据资源和配置将作业拆分成多个并行任务。 
3. 文件写入：每个任务处理其分配到的数据子集，并将结果写入一个或多个文件中。这些文件被创建在HDFS或其他文件系统中，路径通常遵循分区表的结构，例如 `/path/to/table/dt=2023-04-01/`。 
4. 分区目录：所有由同一天的数据生成的文件都会被写入到相同的分区目录下。例如，如果分区键是日期（dt），那么所有2023-04-01的数据都会写入到dt=2023-04-01的分区目录中。 这种并行写入方式可能导致每个分区目录下有多个文件，文件的数量取决于并行任务的数量。如果每个任务处理的数据量很小，就可能产生很多小文件。





### 2.6.3 分区表优化

**背景**

1、报账数据，日生成量千万级，写入MySQL后，会通过binlog写入`mscdeliver0_msc_deliver0_sharding__finance_ebs_data`，然后到`dw_detail_stlplat_finance_ebs_data_all_full`，最后到`fact_finance_ebs_data`。其中`mscdeliver0_msc_deliver0_sharding__finance_ebs_data`表是没有分区的，因此查询性能很差，现在希望查询时，使用`fact_finance_ebs_data`表来操作，因为这张表是进行了分区的

2、查询存量表，同步前45天的数据，写入`dw_detail_stlplat_finance_ebs_data_all_full`。最后会与增量数据进行merge



#### 优化1

1、在 `hmart_mscfinance.invoice_received_tail_diff` 任务中，将查询的表 `mscdeliver0_msc_deliver0_sharding__finance_ebs_data `替换为` fact_finance_ebs_data`，同时使用`spark`引擎执行



**spark比hive默认引擎快的原因**

1. 消除了冗余的HDFS读写。Hadoop每次shuffle操作后，必须写到磁盘，而Spark在shuffle后不一定落盘，可以cache到内存中，以便迭代时使用。如果操作复杂，很多的shufle操作，那么Hadoop的读写IO时间会大大增加。
2. 消除了冗余的MapReduce阶段。Hadoop的shuffle操作一定连着完整的MapReduce操作，冗余繁琐。而Spark基于RDD提供了丰富的算子操作，且action操作产生shuffle数据，可以缓存在内存中。 
3. JVM的优化。Hadoop每次MapReduce操作，启动一个Task便会启动一次JVM，基于进程的操作。而Spark每次MapReduce操作是基于线程的，只在启动Executor时启动一次JVM，内存的Task操作是在线程复用的。 每次启动JVM的时间可能就需要几秒甚至十几秒，那么当Task多了，这个时间Hadoop不知道比Spark慢了多少。 

总结：Spark比Mapreduce运行更快，主要得益于其对mapreduce操作的优化以及对JVM使用的优化。 



#### 优化2

1. `invoice_received_tail_diff`表，分区过多，希望减少分区。此时换成spark引擎即可，因为下边这些参数是针对spark引擎，使用hive默认引擎不会生效。
2. 针对历史的小文件数据，我们通过循环对数据进行冲刷，消除小文件

```sql
## 触发spark合并小文件的阈值(20M) 
SET spark.sql.mergeSmallFileSize=20971520; 
SET spark.hadoopRDD.targetBytesInPartition=67108864; 
SET spark.sql.adaptive.shuffle.targetPostShuffleInputSize=134217728; 
## spark shuffle默认分区数 
SET spark.sql.shuffle.partitions=500;
```



#### 优化3

1. 广播阈值告警问题：两张表在Join的时候会估算各自数据的大小，如果其中有一个表参与Join的数据小于广播阈值，就会将这张表的数据广播，Join策略就是 `BroadcastHashJoin`，在这种Join策略下，即使大表存在数据倾斜，作业仍可以正常执行；
2. 但是如果两张表的数据大小都超过广播阈值，那么两张表的Join策略将会变成 `SortMergeJoin`，在这种情况下，两侧的表都将进行重分区、排序、合并操作，在这种Join策略下，存在数据倾斜的话，作业执行会变得很慢，具体表现就是卡在某个Stage。
3. 有一些XT作业在做上线测试的时候，某两张表的Join策略是 BroadcastHashJoin，随着这两张表数据的不断增大，直到有一天表的数据超过了广播阈值，Join策略变成了 SortMergeJoin。所以我们需要一个告警来通知用户做出判断。



**解决方案：提高广播阈值**

```sql
set spark.sql.autoBroadcastJoinThreshold=104857600;
```





### 2.6.4 Mysql2Hive

从MySQL等关系型数据库的业务数据进行采集，然后导入到Hive中，是进行数据仓库生产的重要环节



一般解决方案是批量取数并Load：直连MySQL去Select表中的数据，然后存到本地文件作为中间存储，最后把文件Load到Hive表中。这种方案的优点是实现简单，但是随着业务的发展，缺点也逐渐暴露出来：

- 性能瓶颈：随着业务规模的增长，Select From MySQL -> Save to Localfile -> Load to Hive这种数据流花费的时间越来越长，无法满足下游数仓生产的时间要求。 
- 直接从MySQL中Select大量数据，对MySQL的影响非常大，容易造成慢查询，影响业务线上的正常服务。 
- 由于Hive本身的语法不支持更新、删除等SQL原语，对于MySQL中发生Update/Delete的数据无法很好地进行支持。 


为了彻底解决这些问题，我们逐步转向CDC (Change Data Capture) + Merge的技术方案，即实时Binlog采集 + 离线处理Binlog还原业务数据这样一套解决方案。Binlog是MySQL的二进制日志，记录了MySQL中发生的所有数据变更，MySQL集群自身的主从同步就是基于Binlog做的。

**整体架构**

![img](https://cdn.nlark.com/yuque/0/2024/png/40369427/1709867597699-bf690c47-1a3d-427a-84c2-28d8dcb41d96.png?x-oss-process=image%2Fformat%2Cwebp)
在Binlog实时采集方面，采用了阿里巴巴的开源项目Canal，负责从MySQL实时拉取Binlog并完成适当解析。Binlog采集后会暂存到Kafka上供下游消费。整体实时采集部分如图中红色箭头所示。

离线处理Binlog的部分，如图中黑色箭头所示，通过下面的步骤在Hive上还原一张MySQL表：

1. 采用Linkedin的开源项目Camus，负责每小时把Kafka上的Binlog数据拉取到Hive上。
2. 对每张ODS表，首先需要一次性制作快照（Snapshot），把MySQL里的存量数据读取到Hive上，这一过程底层采用直连MySQL去Select数据的方式。
3. 对每张ODS表，每天基于存量数据和当天增量产生的Binlog做Merge，从而还原出业务数据。


**为什么用这种方案能解决上面的问题呢？**

- 首先，Binlog是流式产生的，通过对Binlog的实时采集，把部分数据处理需求由每天一次的批处理分摊到实时流上。无论从性能上还是对MySQL的访问压力上，都会有明显地改善。
- 第二，Binlog本身记录了数据变更的类型（Insert/Update/Delete），通过一些语义方面的处理，完全能够做到精准的数据还原。




**Merge**
Binlog成功入仓后，下一步要做的就是基于Binlog对MySQL数据进行还原。

Merge流程做了两件事：

- 首先把当天生成的Binlog数据存放到Delta表中，然后和已有的存量数据做一个基于主键的Merge。Delta表中的数据是当天的最新数据，当一条数据在一天内发生多次变更时，Delta表中只存储最后一次变更后的数据。
- 把Delta数据和存量数据进行Merge的过程中，需要有唯一键来判定是否是同一条数据。如果同一条数据既出现在存量表中，又出现在Delta表中，说明这一条数据发生了更新，则选取Delta表的数据作为最终结果；否则说明没有发生任何变动，保留原来存量表中的数据作为最终结果。Merge的结果数据会Insert Overwrite到原表中，即图中的origindb.*table*。


**Merge举例**

![img](https://cdn.nlark.com/yuque/0/2024/png/40369427/1709867607178-f9cead3c-5f60-41be-82af-c2d3a23dc145.png?x-oss-process=image%2Fformat%2Cwebp)


数据表共id、value两列，其中id是主键。在提取Delta数据时，对同一条数据的多次更新，只选择最后更新的一条。所以对id=1的数据，Delta表中记录最后一条更新后的值value=120。Delta数据和存量数据做Merge后，最终结果中，新插入一条数据（id=4），两条数据发生了更新（id=1和id=2），一条数据未变（id=3）。
默认情况下，我们采用MySQL表的主键作为这一判重的唯一键，业务也可以根据实际情况配置不同于MySQL的唯一键。





# 3、面试问题

1、数据库表的分片键？

- 数据库表的分片键都是**buzLineId**和**customerId**，因为不同的buzLineId下，customerId是可以相同的，例如天津西瓜，北京西瓜，customerId是相同的，但是buzLineId不同

2、如果保证不重复报账？

- 收到上游数据时会做幂等性校验

3、业务数据一定会落库吗？ 会

4、报账数据发送给财务时重复发送了或者发送失败了怎么办？

- 重复发送了月底会对账，下个月月初会对上个月的账单进行关账（基本不会重复，因为有幂等性校验）。失败了会重试，标准化有重试任务表，业财翻译通过拆分为task，以任务为维度进行重试操作

5、为什么不可以让上游直接将业务数据转为财务数据？

- 业务解耦

6、怎么解决报账差异？

- 每天凌晨会通过binlog将前一天数据同步数仓
- 会有对账操作

7、从框架层面分析报账业务。答：标准化、业财翻译

8、如何保证报账数据是正确的？报账的不重不漏

9、怎么保证你保证代码是正确的？有核算验收，同时公司每个月月初会对上个月的账进行关账处理

10、上游业务数据漏发了怎么办？答：感觉应该是上游来保证

11、分了几个库？几个表？

- deliver分了60个库，库中每张表分了64张
- talos分了7个库，一天一张表

12、报账入口，kafka消息和rpc接口，什么场景用

- kafka消息：一般是计费消息，收付款使用。比如微信小店，需要额外付给微信一笔手续费，这种数据走kafka消息
- rpc接口：返现、返佣、通用报账都是走这个接口