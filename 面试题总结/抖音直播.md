1、spring框架最初是解决的什么问题？它是怎么管理bean的生命周期的？

> IOC和AOP。bean管理和实现AOP面向切面编程
>
> 实例化、设置属性值、beanPostProcessor的前置处理方法、init-method、beanPostProcessor的后置处理方法、使用、destroy-method

2、按时间戳去查部分数据，时间较久，可以怎么样优化呢？

> - 为时间戳字段建立索引
> - 给时间戳字段建立索引后，还可以使用覆盖索引

3、为什么数据库用B+树而不用跳表？

> - 磁盘读取效率：B+ 树的设计非常适合磁盘存储。B+ 树的节点包含多个键值对，每个节点的大小通常与磁盘块的大小相匹配，这样可以最大化磁盘读写效率。当进行范围查询时，B+ 树的叶子节点是顺序链接的，可以顺序读取磁盘块，减少磁盘 I/O 操作。
> - 内存使用：跳表需要维护多层索引，这会增加内存消耗。特别是在数据量较大的情况下，跳表的多层索引会占用大量内存。而 B+ 树的索引结构比较紧凑，内存使用相对较少。
> - 稳定性：B+树可以在插入、删除时自动进行平衡，确保树的高度保持相对稳定。跳表需要通过手动调整节点的插入和删除来实现平衡，操作相对复杂。

4、zset为什么用跳表不用平衡树

> - 从内存占用上来比较，跳表比平衡树更灵活一些。平衡树每个节点包含2个指针(分别指向左右子树)，而跳表每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势。
> - 在做范围查找的时候，跳表比平衡树操作要简单。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在跳表上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若千步的遍历就可以实现。
> - 从算法实现难度上来比较，**跳表比平衡树要简单得多**。平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而跳表的插入和删除只需要修改相邻节点的指针，操作简单又快速。
> - redis是纯内存操作，不需要考虑磁盘IO的次数；MySQL为了持久化，需要考虑磁盘IO

5、能不能单独对redis的hash结构设置过期时间？

> Redis中的过期时间是以键的粒度来设置的，无法给hash的某个字段单独设置过期时间

6、Bean的生命周期的过程

7、有一个文件A大小1G，另外有1000个文件B总大小也是1G，为什么删A比删B要更快呢？就好比删1G的代码，老是转圈圈。

> ‌**删除一个大文件比删除多个小文件更快**。删除文件时，文件系统需要进行一系列操作，包括读取目录信息、分配空间、写入目录信息等。对于大量小文件，这些操作需要重复多次，导致总体时间增加。而删除大文件时，这些操作次数较少，因此速度更快‌
>
> Linux文件系统会为每个文件分配两个数据结构：索引节点(index node)和目录项(directory entry)
>
> - 索引节点`inode`，用来记录文件的元信息，比如`inode`编号、文件大小、访问权限、创建时间、修改时间、数据在磁盘的位置等等。索引节点是文件的唯一标识，它们之间一一对应，也同样都会被存储在硬盘中，所以索引节点同样占用磁盘空间。
> - 目录项`dentry`，用来记录文件的名字、索引节点指针以及与其他目录项的层级关联关系。多个目录项关联起来，就会形成目录结构，但它与索引节点不同的是，目录项是由内核维护的一个数据结构，不存放于磁盘，而是缓存在内存。

8、九个小球一个比较轻，有一个天平，问最少称几次能找到轻的球？ 又追问，如果这个小球不知道是重还是轻，最少几次？

> 将9个小球平均分成三组，每组3个。然后，使用天平称量其中的两组。如果天平平衡，说明轻的小球在未被称量的那一组中；如果天平不平衡，则轻的小球在天平上升的那一端。接着，将确定含有轻小球的那组再分成三个小球，使用天平称量其中的两个。如果天平平衡，说明轻的小球是剩下的那一个；如果天平不平衡，则轻的小球在天平上升的一端。这样最少**两次**就能确定哪个小球比较轻‌
>
> 不知道轻重时：2次或4次

9、一个随机数组，求两个数和最大的全部组合数。

> 找到最大数和次大数的个数k1和k2，如果最大数和次大数相等，就是C(k,2)，否则就是k1*k2

10、redis并发度多少才考虑集群

> redis单机QPS可以破10w，如果读多，可以配主从，写多配分片

11、如果有100w的并发，你会怎么设计redis

12、mysql索引树高度怎么算

> 非叶子节点每个索引项：数据大小+指针大小(8字节)，假设索引数据占2字节，那么一页16k/10b≈1600个
>
> 叶子结点存放索引数量：16k/(数据大小+指针)=16k/(2+4+8)≈1100
>
> 1100*1600=1760000

13、给一个数n和一个数组，求用到数组中的数字组成的小于n的最大数

> dfs
>
> https://blog.csdn.net/cuier520/article/details/135196060

~~14、你的项目中客户端调用redis失败了，不能重试，有什么解决方案？~~

15、http1.1和2.0的长连接有什么区别

> http2的长连接有多路复用、头部压缩、二进制格式

~~16、如何设计一个kv键值对的高性能的数据库~~

17、进程和线程所拥有的资源有什么区别

> - 进程是资源（包括内存、打开的文件等）分配的单位，线程是CPU调度的单位
>
> - 进程拥有一个完整的资源平台，而线程只独享必不可少的资源，如寄存器和栈

18、热key是什么问题，怎么解决

> 热点key (hot key)是指在Redis中访问频率非常高的单个或少数几个key。热点数据可能导致Redis服务器的负载不均衡，甚至在高并发场景下成为性能瓶颈。以下是一些设计策略，用于拆分和管理热点数据。JD的hotkey框架，是专门做热key检测的，在单位时间内访问超过设定的阈值频次就是热key，这个阈值需要业务自己设定，并不断的调整和优化。
>
> 1、分散存储：将热点数据根据不同的维度分散存储到不同的key中。例如，如果热点key是按照用户ID进行操作，可以考虑按照用户ID的哈希值或其他规则将数据分布到多个key中。
>
> 2、使用哈希表：如果热点key是由于单个对象过大，可以考虑使用Redis的哈希类型(hash)，将对象的字段分散存储为多个字段。
>
> 3、数据分片(Sharding)：将数据分布到多个Redis实例中。可以使用一致性哈希或范围分片等策略来决定数据应该存储在哪个Redis分片。
>
> 4、键名设计：通过改变键名的设计，例如添加前缀或后缀，可以逻辑上将热点key分散到不同的键上。
>
> 5、限流：对热点key设置访问频率限制，可以使用Redis自带的限流功能或外部的中间件来实现。
>
> 6、使用本地缓存

19、cpu是怎么从内存中读数据的

> CPU Cache 是由很多个 Cache Line 组成的，Cache Line 是CPU 从内存读取数据的基本单位。CPU Cache 的数据是从内存中读取过来的，它是以一小块一小块读取数据的，这一小块数据就称为Cache Line （64字节）

20、数据库隔离级别了解吗，他们本质上有什么区别，分别适用于什么场景

> 读未提交：用于一些对数据一致性要求不高、并发性要求较高的场景，例如某些只读取数据而不进行更新的报表查询或者临时数据分析任务
>
> 读已提交：适用于大多数应用场景，可以避免脏读，但可能存在不可重复读和幻读
>
> 可重复读：适用于要求在事务内多次读取相同数据时结果一致的应用场景
>
> 串行化：适用于对数据一致性要求极高的场景，如金融交易系统

21、读未提交，可能是一个什么场景

22、协程是什么，和线程有什么区别？进程、线程、协程是几比几比几的关系

> - 调度方式：线程的调度由操作系统控制，是抢占式的。而协程的调度通常由开发者控制，更加灵活，可以是协作式的
> - 并发粒度：线程是系统级的并发单元，适用于处理多个相对独立的任务。协程更适用于在单个线程内实现多个任务的协作和切换，粒度更细
> - 上下文切换：线程的上下文切换涉及到内核态和用户态的切换，开销较大。协程的上下文切换通常只在用户态进行，切换速度更快
>
> 一个进程中有多个线程，一个线程中有多个协程

23、算法

- 有序数组的旋转数组，怎么找到数组中最小的数，有比O(N)更快的方法吗

  > lc153

- 多个协程打印0-100，用三种不同的方法实现，无需轮流打印（用java就用线程）

24、cpu是如何知道要去加载内存数据的

> 总线用于 CPU 和内存以及其他设备之间通信
>
> - 地址总线：用于指定 CPU 将要操作的内存地址
> - 数据总线：用于读写内存的数据
> - 控制总线：用于发送和接收信号，比如中断、设备复位等信号，CPU 收到信号后自然进行响应，这时也需要控制总线

25、数据迁移如何做到平滑

> 数据迁移
>
> - 在线迁移：迁移应该是在线的迁移，也就是在迁移的同时还会有数据的写入
> - 数据一致性：数据应该保证完整性，也就是说在迁移之后需要保证新的库和旧的库的数据是一致的
> - 可回滚：迁移的过程需要做到可以回滚，这样一旦迁移的过程中出现问题，可以立刻回滚到源库不会对系统的可用性造成影响
>
> 双写方案
>
> https://blog.csdn.net/JAVA_aik/article/details/140154841
>
> https://blog.csdn.net/pbrlovejava/article/details/125926775
>
> 1、同步。将源库的数据同步到新库上
>
> 2、双写。同步完成后，改造业务代码，写源库的同时，将数据写到新库，写新库可以通过异步的方式，写失败的数据要记录下来。此时存量数据同步完成，需要关闭同步
>
> 3、校验。为了确保双写的数据不出问题，需要通过定时任务来对新库和老库的数据一致性做抽样校验，当校验到数据不一致时，以老库数据为准覆盖新库中的对应数据。
>
> 4、切读。数据校验完成后，数据一致性为100%，可以通过灰度的方式将读请求切到新库，如果出现问题，可以立马切回源库
>
> 5、切写。切读完成没问题后，可以将写请求也切换到新库。然后关闭双写，数据迁移完成

26、详细讲讲一致性哈希，为什么要有一致性哈希，解决了什么问题，环上一个节点挂了怎么办，环上加载一个集群要不要做额外的处理，用了一致性哈希是如何解决数据迁移的问题的

27、青蛙一次跳3-5格，一个数组里面有权重，求跳出去权重最小的值

28、kafka一个实例有一千连接的排查过程说一下

> 1、检查kafka配置，确保Kafka正在监听预期的端口并且客户端能够连接
>
> 2、检查网络，比如防火墙
>
> 3、检查服务器资源是否足够，比如cpu、内存
>
> 4、查看kafka日志文件，查看是否有连接错误信息

29、redlock是如何实现的

30、进程通信的方式有什么

31、对称二叉树